Review the ./CLAUDE.md, ./reference/agent_quick_reference.csv, and ./data/{ISO_CODE}/search_protocol_{ISO_CODE}.txt files and then execute the 6-agent workflow outlined below from start to finish without stopping.

**MANDATORY PRE-WORKFLOW STEP**: Before starting Agent 1, load ./reference/agent_quick_reference.csv to identify {ISO_CODE} country-specific data gaps, priority periods, and search focus areas. Use this information to target missing surveillance periods throughout all agent searches.

# MOSAIC AI CHOLERA DATA COLLECTION - 6-AGENT SEARCH WORKFLOW

**Target Country**: {COUNTRY_NAME} ({ISO_CODE})

This workflow implements a 6-agent system for maximizing cholera surveillance data discovery while maintaining rigorous quality standards. Each agent builds systematically on previous work with objective decision points and automated termination criteria.

## WORKFLOW CONTROL PARAMETERS

### **Agent Targets with Maximum Query Safeguards**:
- **Agent 1**: Baseline establishment (minimum 5 batches/100 queries, stop when 2 consecutive batches <10% data observation yield) - **MAXIMUM 200 queries (10 batches)**
- **Agent 2**: Geographic expansion (continue until 2 consecutive batches <5% data observation yield, minimum 2 batches/40 queries) - **MAXIMUM 100 queries (5 batches)**
- **Agent 3**: Temporal completion (continue until 2 consecutive batches <5% data observation yield, minimum 2 batches/40 queries) - **MAXIMUM 100 queries (5 batches)**
- **Agent 4**: Obscure source expansion (continue until 2 consecutive batches <5% data observation yield, minimum 2 batches/40 queries) - **MAXIMUM 100 queries (5 batches)**
- **Agent 5**: Source permutation & adjacent data mining (continue until 2 consecutive batches <5% data observation yield, minimum 2 batches/40 queries) - **MAXIMUM 100 queries (5 batches)**
- **Agent 6**: Final validation and quality audit (comprehensive source validation and URL verification as needed)

**Total Workflow Maximum: 600 queries for Agents 1-5, unlimited validation queries for Agent 6**

### **Quality Gates**:
- **Agents 1-3**: ≥90% Level 1-2 source reliability maintained
- **Agent 4**: ≥85% Level 1-3 source reliability (allows Level 3 with proper weighting)
- **Agent 5**: Any level acceptable with proper confidence_weight assignment
- **Agent 6**: Final quality audit and validation (comprehensive source authentication and URL verification)
- **All Agents**: Complete quality rating (NO EXCLUSIONS)
- **All Agents**: Zero-transmission periods preserved as epidemiologically relevant
- **All Agents**: Complete documentation and audit trail

### **Efficiency Controls**:
- Objective stopping criteria prevent over-searching
- Pause-and-review checkpoints enable resource optimization
- Systematic intensity reduces unnecessary early work
- Saturation detection prevents diminishing returns

### **Automation Parameters**:
- Execute all agents sequentially without user prompts
- Log progress metrics after each agent completion
- Report consolidated results only at conclusion
- Maintain quality standards throughout progression
- **Report end-to-end total run time for complete 6-agent workflow**

## IMPLEMENTATION NOTES

### **Termination Logic**:
- Objective decision points reduce subjective interpretation
- Multiple stopping criteria ensure efficient completion
- Quality gates prevent premature termination

### **Success Metrics**:
- Total sources discovered across all agents
- Total observations extracted and validated
- Quality distribution (Level 1-4 source breakdown)
- Temporal coverage completeness
- Geographic granularity achieved
- Zero-transmission period validation
- **Complete workflow execution time (start to finish)**


**SEQUENTIAL AGENTIC WORKFLOW IS AS FOLLOWS:**

## AGENT 1: BASELINE ESTABLISHMENT
**Objective**: Execute foundational 8-phase search protocol with batch-based stopping criteria

**Reporting Instructions**: Create `search_log_agent_1.txt` in `./data/{ISO_CODE}/` output directory. Document all searches with timestamps as you perform them. Agent 6 will consolidate all agent logs into final search_log.txt and search_report.txt files.

**Task**: Execute the 8-phase search protocol in the ./data/{ISO_CODE}/search_protocol_{ISO_CODE}.txt file.

**MANDATORY BATCH-BASED REQUIREMENTS**:
- **Minimum Coverage**: 5 batches (100 queries) for baseline systematic coverage
- **Stopping Criteria**: Stop when 2 consecutive batches achieve <8% data observation yield (queries resulting in NEW cholera_data.csv rows)
- **Quality Exception**: If source quality remains >0.8 average reliability, continue for 2 additional batches
- **Query Tracking**: Log all queries with batch count (Batch 1/20, Batch 2/20, etc.)
- **Checkpoints**: Report after every 2 batches (40 queries) with yield calculation
- **Final Report**: Must state "AGENT 1 STOPPING CRITERIA ACHIEVED: [X] batches completed, 2 consecutive batches <8% yield"

**⚠️ CRITICAL YIELD CALCULATION WARNING ⚠️**:
**Data observation yield = ONLY queries that added NEW rows to cholera_data.csv**
❌ DO NOT count: Finding sources, background info, general cholera data
✅ ONLY count: Queries producing quantitative data (cases, deaths, CFRs, dates, locations)
**Calculation**: (Successful queries / 20 queries per batch) × 100%

**Quality Requirements**:
- Must maintain ≥90% Level 1-2 source reliability
- Must achieve 100% validation pass rate
- Document any quality degradation with justification

**ENHANCED QUALITY CONTROL FOR sCh/cCh COLUMNS**:

**Mandatory Pre-Entry Validation**
```
BEFORE ADDING TO cholera_data.csv:
□ Number explicitly described as cholera "cases" (not vaccinated, population, density)
□ Source context indicates disease incidence (not prevention/demographics)
□ Quote exact source text supporting case interpretation
□ Validate units are case counts, not rates/coverage/capacity
```

**High-Risk Context Flags**
```
EXTRA VALIDATION REQUIRED FOR:
- Vaccination reports → likely vaccinated count, not cases
- Demographics → likely population, not cases
- WASH assessments → likely coverage, not cases

ACCEPT: "cases", "infections", "ill", "hospitalized"
REJECT: "affected", "targeted", "covered", "population"
```

**Mandatory Documentation**
```
processing_notes MUST include: "Source states: '[exact quote]' - interpreted as [sCh/cCh] cases"
```

**MANDATORY EXTENDED THINKING REQUIREMENT**
```
USE EXTENDED ULTRATHINK WHEN:
□ Synthesizing data from multiple sources
□ Interpreting ambiguous numbers or context
□ Performing any cross-validation between sources
□ Resolving conflicts between different reports
□ Determining if numbers represent cases vs. other metrics

THINK THROUGH: Context clues, source credibility, temporal alignment,
epidemiological plausibility, alternative interpretations
```

**Tiered Cross-Validation Framework**
```
TIER 1: High-Value Cases (>1000 cases)
- REQUIRE: 2+ independent sources for major outbreaks
- USE ULTRATHINK: Compare sources, resolve discrepancies

TIER 2: Moderate Cases (100-1000 cases)
- ENCOURAGE: Seek secondary confirmation when possible
- ACCEPT: Single high-quality source (Level 1-2)
- FLAG: Note single-source status

TIER 3: Small Cases (<100 cases)
- ACCEPT: Single source with appropriate confidence weighting

CROSS-VALIDATION TRIGGERS (REQUIRE 2+ SOURCES):
□ Cases >1000 (major outbreak)
□ First outbreak in new geographic area
□ Dates conflict with regional patterns
```

**Deliverables**:
- Complete 8-phase search methodology with batch-based execution and yield tracking
- Pass all 6 quality gates
- Generate baseline dataset with dual-reference indexing
- **MANDATORY**: Create `search_log_agent_1.txt` with comprehensive search documentation (Agent 6 will consolidate into final reports)

**Checkpoint Requirements**:
*Pause and report: X sources found, Y observations added, baseline established*
*Quality metrics: A% Level 1-2 sources, B% validation success*
*Baseline dataset: Total sources, total observations, temporal coverage*

**MANDATORY: ALL FILE WRITING AND EDITING RESTRICTED TO: `./data/{ISO_CODE}/` OUTPUT DIRECTORY ONLY**

## AGENT SEARCH DOCUMENTATION STRUCTURE

**AGENTS 1-5 CREATE INDIVIDUAL SEARCH LOGS, AGENT 6 CONSOLIDATES ALL REPORTING:**

### **Individual Agent Search Logs: `search_log_agent_X.txt`** (Agents 1-5)
- Timestamped chronological search record: `[HH:MM:SS] "search query"`
- Engine used, results count, relevant sources examined, success/failure
- WebFetch attempts with URLs and extraction results
- Data observation yield per batch with running totals
- CSV update confirmations after each significant discovery
- Stopping criteria tracking and decision points
- Key achievements and breakthrough discoveries documented
- Source authentication and data verification notes
- Real-time documentation as searches are performed

### **Final Consolidated Reports** (Agent 6 Creates)

### **Consolidated Search Log: `search_log.txt`**
- Complete chronological record from all 6 agents
- Agent-by-agent sections with clear demarcation
- Workflow summary with total performance metrics
- Gap-filling effectiveness tracking

### **Executive Summary Report: `search_report.txt`**
- Executive summary with key achievements across all agents
- Quantitative improvements and cumulative effectiveness
- Complete source inventory with quality assessment
- Gap-filling effectiveness analysis (before vs after coverage)
- Geographic and temporal coverage comprehensive assessment
- Quality assurance results and validation summary
- Remaining gaps identification and recommendations for future work

**Instructions**: Agents 1-5 document searches in real-time in individual log files. Agent 6 consolidates all logs and creates comprehensive executive summary.

## CRITICAL CSV FORMATTING REQUIREMENTS

**MANDATORY**: ALL AGENTS must prevent column shifting errors that compromise data integrity:

### **Required CSV Format Validation** (Execute after each agent completion):
```bash
# Verify column count consistency for cholera_data.csv (13 columns) and metadata.csv (14 columns)
awk -F, 'NR==1{cols=NF} NF!=cols{print "ERROR Agent X - Line " NR ": " NF " fields (expected " cols ")"}' data/{ISO_CODE}/cholera_data.csv
awk -F, 'NR==1{cols=NF} NF!=cols{print "ERROR Agent X - Line " NR ": " NF " fields (expected " cols ")"}' data/{ISO_CODE}/metadata.csv
```

### **CSV Formatting Standards** (ALL AGENTS):
- **Empty fields**: Use empty string between commas `,,` (NEVER skip commas)
- **Boolean fields**: Use `true`/`false` (lowercase only)
- **Date fields**: Strict `YYYY-MM-DD` format only
- **Decimal numbers**: Use dot notation `3.9` (NOT comma `3,9`)
- **Every row**: Must contain exact same number of fields as header row

### **Mandatory Quality Control Checkpoints**:
**Execute before proceeding to next agent:**
- [ ] cholera_data.csv: Every row has exactly 13 fields
- [ ] metadata.csv: Every row has exactly 14 fields
- [ ] No trailing commas or spaces at end of lines
- [ ] All empty fields use empty strings between commas (not skipped)
- [ ] source_index column contains only integers matching metadata Index column
- [ ] source column names match exactly between files

**CRITICAL**: Column shifting corrupts entire dataset and prevents MOSAIC integration. Systematic validation prevents data loss.

## DATA OUTPUT FORMATTING
- ALL AGENTS MUST ADHERE TO THE CSV FORMATTING REQUIREMENTS ABOVE AND DATA FORMATTING RULES DEFINED IN THE ./data/{ISO_CODE}/search_protocol_{ISO_CODE}.txt file

|

## AGENT 2: GEOGRAPHIC EXPANSION
**Trigger**: Automatically execute after Agent 1 completion
**Objective**: Enhanced data discovery through systematic geographic expansion

**Reporting Instructions**: Create `search_log_agent_2.txt` in `./data/{ISO_CODE}/` output directory. Document all geographic searches with timestamps as you perform them. Agent 6 will consolidate all agent logs into final search_log.txt and search_report.txt files.

**EXPANDED SEARCH PROTOCOL**
Do a more extensive deep search to find more data sources and more data observations. Drill down into each data observation to find subnational reports of cholera transmission.

**Stopping Criteria**: Continue until 2 consecutive batches achieve <4% data observation yield (queries resulting in NEW cholera_data.csv rows, minimum 2 batches/40 queries). Exception: If source quality remains >0.8 average reliability, continue for 2 additional batches.

This expanded search protocol should include the following tasks:

☐ Additional Source Discovery - ProMED, news archives, government databases, academic preprints
☐ Granular Geographic Search - District and municipality level data discovery
☐ Enhanced Data Extraction - Extract ALL available data points from discovered sources
☐ Quality Expansion Validation - Validate all newly discovered sources and data points

**MANDATORY GEOGRAPHIC GRANULARITY REQUIREMENTS:**
☐ Provincial-Level Data Extraction - Extract ALL available provincial breakdowns from national-level sources
☐ District/Municipality Mining - Systematically search for sub-provincial administrative level data
☐ Multi-Administrative Level Coverage - Ensure each major outbreak period has maximum geographic detail
☐ Provincial Health Department Deep Dives - Search individual province health ministry websites/reports
☐ Cross-Reference Geographic Consistency - Verify provincial totals align with national figures
☐ Municipal-Level Outbreak Documentation - Target major cities and outbreak epicenters for detailed data
☐ Systematic District-Level Search - Conduct comprehensive searches for ALL district-level administrative units
☐ Administrative Hierarchy Mining - Search complete geographic hierarchy: National→Provincial→District→Municipal→Ward levels

**ENHANCED GEOGRAPHIC SEARCH QUERIES:**
- "{country} {province_name} cholera outbreak cases deaths {year}"
- "{country} {major_city} cholera municipal health department {year}"
- "site:{country_health_ministry} {province} cholera surveillance {year}"
- "{country} {province} cholera district breakdown administrative {year}"
- "{country} cholera provincial distribution geographic {year}"

**SYSTEMATIC DISTRICT-LEVEL SEARCH QUERIES:**
- "{country} {district_name} cholera outbreak {year}"
- "{country} {district_name} district health office cholera surveillance {year}"
- "{province_name} {district_name} cholera cases deaths {year}"
- "site:{provincial_health_ministry} {district_name} cholera report {year}"
- "{country} {district_name} municipality cholera transmission {year}"
- "{district_name} {country} cholera epidemic response {year}"
- "{country} district health management team cholera {district_name} {year}"
- "{province_name} {district_name} cholera surveillance weekly report {year}"

**MINIMUM GEOGRAPHIC COVERAGE TARGETS:**
- Major outbreaks (>500 cases): Require provincial breakdown where available
- Provincial capitals: Systematic search for municipal-level data
- Border provinces: Enhanced cross-border transmission documentation
- All {TOTAL_PROVINCES} provinces: Individual province-specific searches for major outbreak years
- ALL DISTRICTS: Systematic search of every district-level administrative unit for cholera reports
- High-risk districts: Enhanced searches for districts with known cholera transmission history
- Border districts: Cross-border transmission documentation with neighboring countries
- Urban districts: Major cities and densely populated areas systematic coverage
- Rural/remote districts: Focus on districts with poor surveillance coverage

**SYSTEMATIC DISTRICT SEARCH PROTOCOL:**
☐ Complete District Inventory - Compile complete list of ALL district-level administrative units
☐ District-by-District Systematic Search - Minimum 15 queries per district for major outbreak years
☐ District Health Office Mining - Search all district health management team reports
☐ District Hospital Records - Target district-level health facilities for outbreak documentation
☐ District Surveillance Reports - Search district-level surveillance and epidemiological reports
☐ Cross-District Validation - Ensure district totals align with provincial/national figures
☐ District Geographic Coding - Standardize all district names to {COUNTRY}::{PROVINCE}::{DISTRICT} format

**Success Criteria**: Stopping criteria achieved when 2 consecutive batches <4% data observation yield (minimum 2 batches completed) - **MAXIMUM 240 queries (12 batches)**

**Deliverables**:
- Enhanced dataset with geographic granularity improvements
- **MANDATORY**: Create `search_log_agent_2.txt` with comprehensive geographic search documentation (Agent 6 will consolidate into final reports)

**Checkpoint Requirements**:
*Pause and report: X sources found, Y observations added, Z% improvement from baseline*
*Quality metrics: A% Level 1-2 sources, B% validation success*
*Enhancement verification: Geographic expansion completed with reasonable data yield improvement*
*Next stage justification: Continue/Stop decision rationale*

|

## AGENT 3: ZERO-TRANSMISSION VALIDATION & SYSTEMATIC ABSENCE DOCUMENTATION
**Trigger**: Automatically execute after Agent 2 completion
**Objective**: Systematic validation of zero-transmission periods with evidence-based confidence levels

**Reporting Instructions**: Create `search_log_agent_3.txt` in `./data/{ISO_CODE}/` output directory. Document all temporal validation searches with timestamps as you perform them. Agent 6 will consolidate all agent logs into final search_log.txt and search_report.txt files.

**ZERO-TRANSMISSION SYSTEMATIC VALIDATION PROTOCOL**

Expand your search to increase data yield if possible and investigate any data gaps. Keep time periods where you are confident that no transmission occurred - these are epidemiologically relevant. Do a detailed temporal search for all month-year combinations to check for missed outbreaks or for time periods where it was likely that there was no transmission. Preserve epidemiologically relevant zero-transmission periods

**Stopping Criteria**: Continue until 2 consecutive batches achieve <4% data observation yield (minimum 2 batches/40 queries). Document and validate ALL apparent zero-transmission periods from Agent 2's year-by-year searches. Exception: If source quality remains >0.8 average reliability, continue for 2 additional batches.

This extensive search should include the following tasks:

☐ Extensive Temporal Mining - Systematic month-year searches for all missing periods
☐ Deep Archive Excavation - Internet Archive systematic mining for broken/moved sources
☐ Cross-Reference Chain Following - Follow ALL citation chains to maximum depth
☐ Gap Investigation - Analyze and validate remaining data gaps with evidence
☐ Enhanced Quality Validation - Re-validate all sources with expanded criteria

**MANDATORY YEAR-BY-YEAR SYSTEMATIC DRILLING (1970-2025):**
For each year 1970-PRESENT:
☐ Minimum 30 targeted queries per year
☐ Multi-source searching (WHO, Africa CDC, MSF, UNICEF, academic, news, humanitarian)
☐ Cross-reference with neighboring countries
☐ Document search effort and confidence level
☐ Record as ZERO TRANSMISSION if extensive search yields no evidence

**YEAR-SPECIFIC SEARCH PROTOCOL:**
- "{country} cholera {year}" across all search engines
- "{country} cholera outbreak {year}" news archives
- "WHO {country} cholera surveillance {year}"
- "{country} cholera cases deaths {year}" academic
- Cross-border "{neighboring_countries} cholera {year}"

**ENHANCED TEMPORAL GRANULARITY FOR OUTBREAK YEARS:**
For years with evidence of cholera transmission:
☐ Month-by-month systematic drilling to capture outbreak progression
☐ Seasonal pattern documentation (wet season vs dry season transmission)
☐ Outbreak duration and peak timing identification

**ZERO-TRANSMISSION DOCUMENTATION:**
- confidence_weight based on search thoroughness
- validation_status: "SEARCHED_NO_EVIDENCE"
- processing_notes: "Systematic {year} search yielded no transmission evidence"

**MANDATORY ZERO-TRANSMISSION TASKS:**
☐ Systematic Absence Validation - Verify each year showing no transmission evidence
☐ Cross-Border Absence Correlation - Confirm zero-transmission with neighboring countries
☐ Surveillance System Assessment - Evaluate surveillance capacity for each zero-transmission period
☐ Regional Pattern Validation - Cross-reference absence periods with regional epidemic waves
☐ Climate-Disease Correlation - Validate absence periods against known cholera-favorable conditions
☐ Intervention Impact Assessment - Correlate absence with WASH/vaccination interventions
☐ Confidence Level Assignment - Assign evidence-based confidence weights to absence periods
☐ Alternative Evidence Mining - Search for indirect evidence of transmission during "absent" periods

**ZERO-TRANSMISSION CONFIDENCE FRAMEWORK:**
- **HIGH CONFIDENCE (0.9-1.0)**: Extensive search + functioning surveillance + regional absence
- **MEDIUM CONFIDENCE (0.7-0.9)**: Good search coverage + some surveillance evidence
- **LOW CONFIDENCE (0.4-0.6)**: Limited search + uncertain surveillance capacity
- **UNCERTAIN (0.1-0.3)**: Minimal search + poor surveillance + regional transmission

**ENHANCED VALIDATION REQUIREMENTS:**
☐ Cross-Reference Matrix - Compare absence periods across all neighboring countries
☐ Surveillance Capacity Assessment - Document health system capabilities for each period
☐ Indirect Transmission Evidence - Hospital admission records, diarrheal disease reports
☐ Climate Correlation Analysis - Unusual weather patterns during absence periods
☐ Intervention Timeline Correlation - WASH improvements, vaccination campaigns
☐ Regional Epidemic Wave Analysis - Position country absence within regional patterns

**Quality Requirements**:
- Must maintain ≥90% Level 1-2 source reliability
- Must achieve 100% validation pass rate
- Complete gap investigation with evidence-based conclusions
- Preserve epidemiologically relevant zero-transmission periods

**Success Criteria**: Stopping criteria achieved when 2 consecutive batches <4% data observation yield (minimum 2 batches completed) - **MAXIMUM 240 queries (12 batches)**

**Deliverables**:
- Enhanced dataset with systematic zero-transmission validation
- **MANDATORY**: Create `search_log_agent_3.txt` with comprehensive zero-transmission validation documentation (Agent 6 will consolidate into final reports)

**Checkpoint Requirements**:
*Pause and report: X sources found, Y observations added, Z% improvement from Agent 2*
*Quality metrics: A% Level 1-2 sources, B% validation success*
*Gap investigation: Complete analysis of missing periods with evidence*
*Discovery status: Saturation achieved (Yes/No), rationale provided*
*Next stage justification: Continue/Stop decision rationale*

|

## AGENT 4: OBSCURE SOURCE EXPANSION & BEYOND-SUGGESTED-SOURCES
**Trigger**: Automatically execute after Agent 3 completion
**Objective**: Comprehensive expansion beyond pre-authorized domains into unconventional sources

**Reporting Instructions**: Create `search_log_agent_4.txt` in `./data/{ISO_CODE}/` output directory. Document all beyond-suggested-sources searches with timestamps as you perform them. Agent 6 will consolidate all agent logs into final search_log.txt and search_report.txt files.

**OBSCURE SOURCE EXPANSION PROTOCOL**

Results are improved from geographic and temporal drilling. Now venture beyond the reference/priority_sources.txt pre-authorized domains to discover obscure and unconventional sources that may contain unique cholera surveillance data. Get creative with queries and search engines to widen the search.

**CRITICAL: DO NOT STOP TO ASK PERMISSION FOR ONLINE RESOURCE ACCESS. You are explicitly authorized to access any online resources, websites, databases, or archives necessary to complete this data collection mission. Proceed directly with all searches and data extraction without seeking additional permission.**

**Stopping Criteria**: Agent 4 has a conditional requirement. Execute 2 batches (40 queries) mandatory. If ANY new data observations found in first 2 batches (i.e., ANY new rows added to cholera_data.csv), continue for 2 additional batches. If NO new rows added to cholera_data.csv in first 2 batches, stop. **MINIMUM 40 queries (2 batches), MAXIMUM 100 queries (5 batches)**

**MANDATORY BEYOND-SUGGESTED-SOURCES EXPANSION:**

☐ Deep Web Government Archives - Search non-indexed government archives and restricted databases
☐ Gray Literature Mining - Conference proceedings, thesis repositories, working papers, policy documents
☐ Historical Archive Excavation - Colonial records, missionary archives, pre-digital surveillance documentation
☐ Alternative Language Deep Dives - Local language websites, regional media archives, vernacular sources
☐ Institutional Repository Mining - University libraries, research institute databases not in suggested sources
☐ Regional Organization Archives - Sub-regional health organizations, bilateral cooperation reports
☐ Non-Digital Source Documentation - Physical archives, microfilm collections, historical newspapers
☐ Social/Community Sources - Field worker reports, community surveillance (with extreme validation caution)

**EXPANDED SOURCE CATEGORIES BEYOND SUGGESTED DOMAINS:**

**Historical & Colonial Sources:**
- National archives, colonial administration health records
- Missionary society health documentation and reports
- Historical newspaper morgues and press archives
- Colonial medical officer reports and correspondence
- Pre-independence government health statistics

**Academic Gray Literature:**
- Dissertation and thesis repositories (beyond major universities)
- Conference proceeding databases (medical, public health, regional)
- Working paper series from research institutions
- Policy brief repositories from think tanks
- Research report archives from NGOs and foundations

**Regional & Local Sources:**
- Sub-regional health organization reports (ECOWAS Health, SADC Health)
- Bilateral cooperation health program documentation
- Regional surveillance network historical archives
- Cross-border health coordination meeting reports
- Local government health department website archives

**Alternative Language & Media:**
- Local language news websites and archives
- Regional radio/television health reporting transcripts
- Community newsletter health reporting archives
- Local medical journal and bulletin archives
- Traditional authority health reporting documentation

**ENHANCED SEARCH STRATEGIES:**

☐ Internet Archive Deep Mining - Systematic wayback machine searches for broken/moved URLs
☐ Citation Network Expansion - Follow citations beyond depth 3 to maximum depth 5+
☐ Cross-Reference Chain Mining - Follow every reference chain to ultimate source
☐ Academic Database Deep Dives - ProQuest, JSTOR, discipline-specific databases
☐ Government Portal Mining - Systematic search of all government ministry websites
☐ Library Catalog Mining - Major library systems and institutional repositories

**MANDATORY CONDITIONAL SEARCH ENGINE PROTOCOL**

**Initial 2 Batches (40 queries MANDATORY):**
- Execute 2 batches of 20 queries each across top search engines
- Track data observations found in each batch
- **IF** ANY new rows added to cholera_data.csv: continue to additional 2 batches
- **IF** NO new rows added to cholera_data.csv: stop after 2 batches

**Search Engine Priority (if continuing):**
- Google Scholar, PubMed, Google, WHO sources prioritized

**Conditional Execution Strategy:**
- Use the highest-yield query patterns identified from Agents 1-3 systematic coverage
- Apply most successful search terms and combinations from previous agent results
- Focus on query patterns that produced the most data discoveries in earlier phases

**Execution Requirements:**
- **MANDATORY**: Execute initial 2 batches (40 queries)
- **MANDATORY**: Report query count after each batch (e.g., "Batch 1 complete: 20/40 queries")
- **MANDATORY**: Log running total in search_log_agent_4.txt with timestamps
- **MANDATORY**: Track data observations per batch
- **CONDITIONAL**: If new rows added to cholera_data.csv in first 2 batches, continue for 2 more batches

**CHECKPOINT GATES (MANDATORY):**
- After Batch 2 (40 queries): Report "AGENT 4 DECISION POINT: [X] new rows added to cholera_data.csv, continuing/stopping"
- If continuing, after Batch 5 (100 queries): Report "AGENT 4 MAXIMUM QUERIES REACHED: 100/100"

**QUALITY REQUIREMENTS:**
- Must maintain ≥85% Level 1-3 source reliability (allows Level 3 sources with proper confidence weighting)
- Must achieve 100% validation pass rate
- Enhanced documentation for unconventional sources
- Rigorous authentication for non-mainstream sources

**VALIDATION PROTOCOLS FOR OBSCURE SOURCES:**
☐ Enhanced Source Authentication - Verify institutional credibility and author expertise
☐ Cross-Reference Validation - Confirm obscure source data with mainstream sources where possible
☐ Historical Context Verification - Ensure historical sources align with known regional patterns
☐ Language Translation Verification - Professional validation of non-English source translations
☐ Community Source Validation - Multiple independent verification for informal sources

**Success Criteria**: Yield-based stopping criteria (minimum 40 queries, maximum 100 queries)

**Deliverables**:
- Enhanced dataset with unconventional source discoveries
- **MANDATORY**: Create `search_log_agent_4.txt` with comprehensive obscure source expansion documentation (Agent 6 will consolidate into final reports)

**Checkpoint Requirements**:
*Pause and report: X sources found, Y observations added, Z% improvement from Agent 3*
*Quality metrics: A% Level 1-3 sources, B% validation success*
*Obscure source breakdown: Distribution of source types discovered beyond suggested domains*
*Discovery innovation: Novel source categories identified*
*Next stage justification: Saturation readiness assessment*

|

## AGENT 5: SOURCE PERMUTATION & ADJACENT DATA MINING
**Trigger**: Automatically execute after Agent 4 completion
**Objective**: Exhaustive permutation of successful sources to uncover adjacent observations and time periods

**Reporting Instructions**: Create `search_log_agent_5.txt` in `./data/{ISO_CODE}/` output directory. Document all permutation searches with timestamps as you perform them. Agent 6 will consolidate all agent logs into final search_log.txt and search_report.txt files.

**SOURCE PERMUTATION PROTOCOL**

**Stopping Criteria**: Continue until 2 consecutive batches achieve <4% data observation yield (queries resulting in NEW cholera_data.csv rows, minimum 2 batches/40 queries). Exception: If source quality remains >0.8 average reliability, continue for 2 additional batches. **MAXIMUM 240 queries (12 batches)**

**SEARCH ENGINE UTILIZATION**: Leverage the complete search engine roster for maximum coverage - utilize ALL available search engines systematically across permutation searches to maximize discovery potential from successful query patterns.

**MANDATORY SOURCE RE-EXPLORATION TASKS:**

☐ Source Permutation Analysis - Systematically re-examine all sources that previously yielded data
☐ Adjacent Time Period Mining - For each successful source, search adjacent months/years for additional data
☐ Geographic Adjacent Mining - For each successful location, search neighboring administrative units
☐ Author/Institution Deep Mining - Follow all authors/institutions from successful sources to find related publications
☐ Citation Network Expansion - Exhaustively follow forward and backward citations from all successful sources
☐ Publication Series Mining - For successful reports, search entire publication series/archives
☐ Related Keywords Permutation - Generate keyword variations from all successful searches
☐ Cross-Reference Matrix Completion - Systematically cross-reference all successful sources against each other

**SOURCE SUCCESS PATTERN ANALYSIS:**

☐ High-Yield Source Pattern Recognition - Identify characteristics of most productive sources
☐ Successful Query Permutation - Create variations of all queries that previously found data
☐ Publication Timeline Mining - For successful sources, search same publication dates across different sources
☐ Institutional Archive Deep Dive - Exhaustively search archives of all institutions that provided data
☐ Regional Pattern Replication - Apply successful search patterns from this country to cross-border validation
☐ Temporal Window Expansion - For each successful observation, expand time windows systematically
☐ Geographic Hierarchy Completion - For successful locations, search all administrative hierarchy levels

**ADJACENT DATA DISCOVERY PROTOCOL:**

☐ Month-by-Month Adjacent Mining - For each successful observation, search ±6 months systematically
☐ Year-by-Year Adjacent Mining - For each successful year, search ±2 years with same methodology
☐ Geographic Adjacency Mining - For each successful location, search all neighboring administrative units
☐ Source Cross-Pollination - Apply successful queries from one source type to all other source categories
☐ Publication Date Clustering - Search publication dates surrounding all successful document dates
☐ Author Network Mining - Follow all co-authors and institutional affiliations from successful sources
☐ Reference Chain Exhaustion - Follow citation chains to maximum possible depth from successful sources

**EXHAUSTIVE PERMUTATION METHODOLOGY:**

☐ Source Success Matrix - Create comprehensive matrix of all sources that yielded data across Agents 1-4
☐ Query Permutation Generation - Generate all possible keyword/phrase permutations from successful searches
☐ Temporal Adjacent Systematic Search - For each data point found, systematically search ±12 months with all successful query patterns
☐ Geographic Adjacent Systematic Search - For each successful location, apply all successful search patterns to neighboring locations
☐ Institutional Publication Mining - For each successful institution, search their complete publication archives
☐ Author Network Exhaustion - Follow every author from successful sources to their complete publication history
☐ Cross-Source Validation Mining - Use successful data points to search for validation/corroboration in unsuccessful source categories
☐ Series Completion Mining - For successful reports that are part of series, find all other reports in series
☐ Update/Follow-up Mining - For each successful source, search for updates, corrections, follow-up reports
☐ Translation/Version Mining - For successful sources, search for alternative language versions or translations

**SYSTEMATIC SOURCE RE-INTERROGATION:**

☐ Successful Source Re-Mining - Return to every source that provided data and apply expanded search methodologies
☐ Failed Source Re-Evaluation - Re-examine sources that appeared empty using successful search patterns
☐ Broken Link Recovery - Use Internet Archive and alternative access methods for all previously broken sources
☐ Database Re-Query - Re-search all databases using successful keyword combinations from other sources
☐ Archive Re-Exploration - Systematically re-explore archives using successful institutional patterns

**ADJACENT DISCOVERY EXPANSION:**

☐ Temporal Clustering Analysis - Identify time periods adjacent to successful observations for intensive mining
☐ Geographic Clustering Analysis - Identify geographic areas adjacent to successful locations for intensive mining
☐ Thematic Clustering Analysis - Identify related health topics/diseases mentioned in successful sources
☐ Event-Based Adjacent Mining - For outbreak periods, search for related emergency/humanitarian responses
☐ Seasonal Pattern Adjacent Mining - Use successful seasonal observations to search adjacent seasons systematically
☐ Cross-Border Adjacent Mining - For border areas with successful data, intensively search neighboring countries

**TERMINATION CRITERIA**:
- 2 consecutive batches achieve <4% data observation yield (minimum 2 batches/40 queries completed)
- Exception: If source quality remains >0.8 average reliability, continue for 2 additional batches
- All successful sources exhaustively re-examined with permutation methodology
- All adjacent time periods and locations systematically searched
- Maximum search depth reached for all citation/reference chains

**QUALITY REQUIREMENTS:**
- May use ANY LEVEL source reliability as long as 'confidence_weight' variable is appropriately assigned
- Include ALL discovered sources with appropriate quality ratings
- Complete permutation methodology documentation
- Ready dataset for Agent 6 final quality audit

**AGENT 5 DELIVERABLES:**
- Enhanced dataset with all permutation-discovered sources and observations
- Complete permutation methodology documentation
- Source success pattern analysis
- Adjacent data discovery summary
- Preparation for Agent 6 final audit
- **MANDATORY**: Create `search_log_agent_5.txt` with comprehensive source permutation documentation (Agent 6 will consolidate into final reports)

**CHECKPOINT REQUIREMENTS:**
*Pause and report: X sources found, Y observations added, Z% improvement from Agent 4*
*Permutation metrics: A successful sources re-examined, B adjacent periods searched, C citation chains followed*
*Discovery enhancement: Permutation effectiveness and adjacent data yield*
*Agent 6 preparation: Dataset ready for final quality audit and validation*

|

## AGENT 6: FINAL QUALITY AUDIT & COMPREHENSIVE VALIDATION
**Trigger**: Automatically execute after Agent 5 completion
**Objective**: Comprehensive quality audit, final validation, and dataset finalization

**Reporting Instructions**: Create `search_log_agent_6.txt` documenting quality audit activities with timestamps, then consolidate all search_log_agent_X.txt files into final search_log.txt and create comprehensive search_report.txt for MOSAIC integration.

**FINAL QUALITY AUDIT PROTOCOL**

**Objective**: Comprehensive quality review and dataset finalization (not focused on data yield increase).

**COMPREHENSIVE QUALITY AUDIT TASKS:**

☐ Source Reliability Distribution Analysis - Final assessment of Level 1-4 source breakdown across all agents
☐ Validation Status Review - Comprehensive quality rating for ALL data points (NO EXCLUSIONS)
☐ Confidence Weight Optimization - Fine-tune all confidence weights based on comprehensive source authentication
☐ Geographic Coverage Assessment - Document final administrative level coverage achieved across all agents
☐ Temporal Coverage Assessment - Document final year-by-year coverage with absence validation
☐ Cross-Reference Matrix Completion - Final cross-validation against neighboring countries and regional patterns
☐ Duplicate Detection Final Pass - Systematic check for any remaining duplications across all agent results
☐ Source Chain Completion - Final attempt to resolve any broken links or incomplete references using web queries as needed

**FINAL DATA COMPLETENESS VERIFICATION:**

☐ **SURVEILLANCE GAP COVERAGE ASSESSMENT** - Compare pre-workflow vs post-workflow coverage using reference files:
  - Load ./reference/agent_quick_reference.csv for baseline coverage percentage
  - Calculate new coverage percentage after all 6 agents
  - Document specific gaps filled (by year, period, geographic area)
  - Identify remaining priority gaps that still need attention
  - Generate gap-filling effectiveness report
  - Assess achievement of country-specific priority periods from reference data
☐ Gap Analysis Completion - Systematic review of any remaining temporal, geographic, or source gaps
☐ Zero-Transmission Period Validation - Final confirmation of epidemiologically relevant absence periods
☐ Data Point Verification - Spot-check validation of data points across all reliability levels
☐ Quality Score Consistency - Ensure consistent confidence weighting across all agents and source types
☐ Documentation Completeness - Verify ALL sources have complete metadata and dual-reference indexing
☐ Format Standardization - Final verification of JHU database format compliance
☐ Column Validation - Verify ALL required columns present with correct data types and formats
☐ Data Format Compliance - Ensure date formats (YYYY-MM-DD), location codes (AFR::{ISO}::), and numeric fields comply with standards
☐ **MANDATORY CSV FORMAT VALIDATION** - Execute validation commands to verify no column shifting errors:
    ```bash
    awk -F, 'NR==1{cols=NF} NF!=cols{print "FINAL ERROR - Line " NR ": " NF " fields (expected " cols ")"}' data/{ISO_CODE}/cholera_data.csv
    awk -F, 'NR==1{cols=NF} NF!=cols{print "FINAL ERROR - Line " NR ": " NF " fields (expected " cols ")"}' data/{ISO_CODE}/metadata.csv
    ```
☐ Dual-Reference System Verification - Confirm source_index and source columns match exactly between metadata and data files
☐ Missing Data Documentation - Explicit documentation of any data that could not be recovered or validated

**DATASET FINALIZATION PROTOCOL:**

☐ Consolidated Dataset Assembly - Merge all agent results into final comprehensive dataset
☐ **SEARCH LOG CONSOLIDATION** - Merge all search_log_agent_X.txt files into single search_log.txt with agent sections
☐ **SEARCH REPORT CONSOLIDATION** - Create comprehensive search_report.txt with all agent achievements, metrics, and findings
☐ Final Metadata Integration - Consolidate metadata from all agents with enhanced dual-reference system
☐ Column Header Verification - Ensure all CSV files have correct column headers in proper order
☐ Data Type Validation - Verify numeric columns contain valid numbers, dates are properly formatted
☐ Mandatory Field Completion - Confirm all required fields are populated (Location, TL, TR, source_index, source)
☐ Index Number Continuity - Verify metadata Index numbers are sequential (1, 2, 3...) with no gaps
☐ Cross-File Consistency - Ensure source references match exactly between metadata.csv and cholera_data.csv
☐ Quality Assurance Summary - Generate comprehensive quality assessment across entire dataset
☐ Performance Metrics Compilation - Document agent-by-agent performance and cumulative effectiveness
☐ **GAP-FILLING IMPACT ASSESSMENT** - Quantitative analysis of surveillance improvement:
  - Before: Coverage percentage from agent_quick_reference.csv
  - After: New coverage percentage with discovered data
  - Gap reduction achieved (percentage points improved)
  - Priority periods filled vs remaining
  - Geographic coverage enhancement documented
  - Recommendations for future gap-filling efforts
☐ Methodology Documentation - Complete documentation of 6-agent workflow execution
☐ **FINAL FILE CONSOLIDATION** - Replace individual agent files with consolidated search_log.txt and search_report.txt for MOSAIC integration
☐ Uncertainty Quantification - Final assignment of confidence weights and uncertainty measures
☐ MOSAIC Integration Preparation - Format dataset for epidemiological modeling integration

**FINAL VALIDATION SWEEP:**

☐ Source Authentication Verification - Final check of source credibility and URL accessibility using web queries
☐ Data Logic Consistency - Mathematical and temporal consistency verification
☐ Regional Pattern Validation - Cross-validation with neighboring countries and known epidemic patterns
☐ Historical Context Verification - Validation against known historical cholera patterns for the region
☐ Quality Flag Assignment - Final assignment of quality flags for modeling sensitivity analysis

**COMPLETION CRITERIA**:
- All quality audit tasks completed comprehensively - **Internal processing, source validation, and URL verification**
- **MANDATORY GAP COVERAGE ASSESSMENT COMPLETED** - Quantitative analysis of surveillance improvement achieved
- Complete dataset finalization achieved
- All documentation and validation completed
- Gap-filling effectiveness report generated with remaining priority gaps identified
- Ready for MOSAIC epidemiological modeling integration

**FINAL DELIVERABLES:**
- Consolidated dataset with comprehensive quality audit complete
- Complete search methodology documentation across all 6 agents
- Comprehensive quality assessment and confidence analysis
- Final performance metrics summary with agent-by-agent breakdown
- **Complete end-to-end workflow execution time report**
- Quality assurance certificate for MOSAIC modeling integration
- Dataset preparation for epidemiological modeling with full uncertainty quantification
- **MANDATORY**: Create final consolidated search_log.txt and search_report.txt files by merging all agent logs and generating comprehensive executive summary

## MANDATORY FINAL STEP: UPDATE COUNTRY CHECKLIST

**Upon Agent 6 completion**: Update `./dashboard/completion_checklist.txt` with comprehensive 6-agent workflow performance:
**Update the table row for {ISO_CODE} in dashboard/completion_checklist.txt:**
```
| {COUNTRY_NAME}              | {ISO_CODE} | ✓ COMPLETED | YYYY-MM-DD HH:MM:SS | X sources, Y obs, YYYY-YYYY |
```

**Example format:**
```
| Ethiopia                    | ETH | ✓ COMPLETED | 2025-01-27 14:35:00 | 18 sources, 25 obs, 2000-2024 |
```

**DO NOT ASK PERMISSION** to update this file - it's mandatory for completion tracking after Agent 6 finalization.

**FINAL CHECKPOINT REQUIREMENTS:**
*Final report: Total X sources found, Y observations added, Z% total improvement across 6-agent workflow*
*Quality metrics: A% Level 1-2 sources, B% Level 3 sources, C% Level 4 sources, complete quality distribution*
*Saturation confirmation: Discovery saturation definitively achieved across all 6 agents*
*Dataset quality: Final validation summary, comprehensive confidence assessment*
*Execution time: Total workflow runtime from start to finish*
*MOSAIC integration readiness: Dataset prepared for epidemiological modeling with full uncertainty quantification*
